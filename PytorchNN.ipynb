{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7000ef6a",
   "metadata": {},
   "source": [
    "# Redes Neurais Artificiais com PyTorch\n",
    "# Sumário\n",
    "\n",
    "[1. Operações com Tensores](#heading--1)\n",
    "\n",
    "[2. DataSets e DataLoaders](#heading--2)\n",
    "\n",
    "   * [2.1 Carregando um DataSet](#heading--2-1)\n",
    "    \n",
    "   * [2.2 Criando um DataSet](#heading--2-2)\n",
    "    \n",
    "   * [2.3 Preparando os Dados para Treino com DataLoaders](#heading--2-3)\n",
    "   \n",
    "[3. Construindo uma Rede Neural](#heading--3)\n",
    "\n",
    "   * [3.1 Definindo a Classe](#heading--3-1)\n",
    "   * [3.2 Camadas e Parâmetros](#heading--3-2)\n",
    "   \n",
    "[4. Diferenciação Automática com __torch.autograd__](#heading--4)\n",
    "\n",
    "   * [4.1 Computando Gradientes](#heading--4-1)\n",
    "   * [4.1 Desabilitandi o Rastreamento dos gradientes](#heading--4-2)\n",
    "\n",
    "[5. Treninando e Testando o Modelo](#heading--5)\n",
    "\n",
    "   * [5.1 Hiperparâmetros](#heading--5-1)\n",
    "   * [5.1 Funções de Perda e Otimizador](#heading--5-2)\n",
    "   * [5.1 Implementação](#heading--5-3)\n",
    "   * [5.1 Salvando e Carregando um Modelo](#heading--5-4)\n",
    "\n",
    "<a id=\"heading--1\"></a>\n",
    "## Operações com Tensores\n",
    "Os tensores no PyTorch são matrizes multidimensionais que representam dados. Eles são semelhantes aos arrays do Numpy, mas têm algumas vantagens, como poder ser operados em GPUs e armazenar o histórico de cálculos para facilitar a diferenciação automática.\n",
    "\n",
    "Os tensores podem ser criados a partir de listas Python, arrays Numpy ou valores aleatórios usando a classe torch.Tensor ou suas subclasses2. Os tensores são a base para o aprendizado profundo com PyTorch, pois permitem a construção e o treinamento de redes neurais complexas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea05c230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b64e736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor aleatório:\n",
      " tensor([[0.9095, 0.4715, 0.8731, 0.2251],\n",
      "        [0.6308, 0.3806, 0.0483, 0.8550],\n",
      "        [0.9168, 0.1286, 0.8631, 0.9469]])\n",
      "\n",
      "Tensor preenchido com 1s:\n",
      " tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "\n",
      "\n",
      "Indexação e Fatiamento:\n",
      "\n",
      "Primeira Linha: tensor([1., 1., 1., 1.])\n",
      "Primeira Coluna: tensor([1., 1., 1., 1.])\n",
      "Última Coluna: tensor([1., 1., 1., 1.])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n",
      "Concatenação:\n",
      "\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "tensor1 = torch.ones(2,2)\n",
    "\n",
    "print(\"Tensor aleatório:\\n\", tensor)\n",
    "print(\"\\nTensor preenchido com 1s:\\n\", tensor1)\n",
    "print(\"\\n\")\n",
    "\n",
    "#Indexação e fatiamento semelhante a Numpy\n",
    "\n",
    "print(\"Indexação e Fatiamento:\\n\")\n",
    "tensor = torch.ones(4, 4)\n",
    "print(f\"Primeira Linha: {tensor[0]}\")\n",
    "print(f\"Primeira Coluna: {tensor[:, 0]}\")\n",
    "print(f\"Última Coluna: {tensor[..., -1]}\")\n",
    "tensor[:,1] = 0\n",
    "print(tensor)\n",
    "\n",
    "print(\"Concatenação:\\n\")\n",
    "t1 = torch.cat([tensor, tensor, tensor], dim=0)    #dimensão do novo vetor\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d652876e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matriz Transposta:\n",
      " tensor([[1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "\n",
      "Mutiplicação Matricial:\n",
      " tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "#Mutiplicação Matricial\n",
    "y0 = tensor.T\n",
    "\n",
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "print(\"\\nMatriz Transposta:\\n\", y0)\n",
    "print(\"\\nMutiplicação Matricial:\\n\", y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3370986a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiplicação termo a termo:\n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "#Multiplicação termo a termo\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "print(\"Multiplicação termo a termo:\\n\", z2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fa513a",
   "metadata": {},
   "source": [
    "<a id=\"heading--2\"></a>\n",
    "## DataSets e DataLoaders\n",
    "\n",
    "O código para processar amostras de dados pode ficar confuso e difícil de manter; idealmente, queremos que nosso código de conjunto de dados seja desacoplado do nosso código de treinamento de modelo para melhor legibilidade e modularidade. \n",
    "\n",
    "*Dataset* armazena as amostras e seus rótulos correspondentes, e *DataLoader* envolve um iterável em torno do *Dataset* para permitir um fácil acesso às amostras. PyTorch fornece duas primitivas de dados: **torch.utils.data.DataLoader** e __torch.utils.data.Dataset__ que permitem que você use conjuntos de dados pré-carregados, bem como seus próprios dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294720a6",
   "metadata": {},
   "source": [
    "<a id=\"heading--2-1\"></a>\n",
    "### Carregando um DataSet\n",
    "\n",
    "Aqui está um exemplo de como carregar o conjunto de dados Fashion-MNIST do TorchVision1. Fashion-MNIST é um conjunto de dados de imagens de artigos da Zalando, consistindo de 60.000 exemplos de treinamento e 10.000 exemplos de teste2. Cada exemplo é composto por uma imagem em escala de cinza de 28×28 e um rótulo associado de uma das 10 classes.\n",
    "\n",
    "Carregamos o conjunto de dados FashionMNIST com os seguintes parâmetros: \n",
    "\n",
    "**root** é o caminho onde os dados de treino/teste são armazenados,\n",
    "\n",
    "**train** especifica o conjunto de dados de treinamento ou teste,\n",
    "\n",
    "**download=True** baixa os dados da internet se eles não estiverem disponíveis em root.\n",
    "\n",
    "**transform** e **target_transform** especificam as transformações de características e rótulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb5b0117",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad09bae",
   "metadata": {},
   "source": [
    "<a id=\"heading--2-2\"></a>\n",
    "### Criando um Dataset\n",
    "Para a riação de um objeto que herde a classe Dataset, é necessário a utilização de três funções:\n",
    "\n",
    "__init__: Fornece o conjunto de dados para a calsse;\n",
    "__len__: Retorna o tamanho do conjunto de dados;\n",
    "__getitem__: Retorna o dado de indíce especificado.\n",
    "\n",
    "Os dados podem ser armazenados em arquivos externos como arquivos CSV, que contém tanto o conjunto de dados como a etiqueta dos dados. Também é possível passar parâmetros que representem funções, para geração de dados na própria criação da classe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0e557b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torch.distributions import Normal\n",
    "class Dados(Dataset):\n",
    "    def __init__(self, media, desvio, qamostras):\n",
    "        # Cria uma distribuição normal com média 0 e desvio padrão 1\n",
    "        self.distribuicao = Normal(torch.tensor([0.0]), torch.tensor([1.0]))\n",
    "\n",
    "        # Gera dez amostras aleatórias\n",
    "        self.amostras = self.distribuicao.sample((qamostras,))\n",
    "        \n",
    "        self. alvos = torch.rand(self.amostras.shape)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.amostras)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.amostras[i], self.alvos[i]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c0c95c",
   "metadata": {},
   "source": [
    "<a id=\"heading--2-3\"></a>\n",
    "### Preparando os Dados para Treino com DataLoaders\n",
    "\n",
    "É interesssante que para o treinamento do modelo, os dados sejam passados em lotes, e a cada _época_, eles sejam reorganizados, para evitar o overfitting - Situação indesejável na qual o modelo se adequa perfeitamente aos dados, mas não consegue generalizar para outros conjuntos de dados, tornando o modelo inútil -, e usar o multiprocessamento do Python para acelerar a recuperação de dados.\n",
    "\n",
    "DataLoader é um iterável que faz justamente esse processo, em uma API(Interface de Programação de Aplicações) fácil de usar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85e4ee13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 1])\n",
      "torch.Size([200, 1])\n",
      "Batch: 0, Data: [tensor([[-0.5707],\n",
      "        [ 0.2190],\n",
      "        [-1.3237],\n",
      "        [ 1.6763],\n",
      "        [ 0.5365],\n",
      "        [-0.1446],\n",
      "        [ 0.2325],\n",
      "        [ 0.8397],\n",
      "        [-1.2482],\n",
      "        [ 0.1841],\n",
      "        [ 0.9017],\n",
      "        [-0.6452],\n",
      "        [-0.2536],\n",
      "        [ 0.3142],\n",
      "        [ 0.6364],\n",
      "        [ 1.0160],\n",
      "        [ 0.6792],\n",
      "        [-1.1867],\n",
      "        [-0.0431],\n",
      "        [ 0.2760],\n",
      "        [-0.3222],\n",
      "        [-0.1634],\n",
      "        [ 0.3855],\n",
      "        [ 1.5882],\n",
      "        [-0.7877],\n",
      "        [-1.4374],\n",
      "        [ 0.4654],\n",
      "        [ 1.0757],\n",
      "        [-0.7070],\n",
      "        [-3.1457],\n",
      "        [-2.0231],\n",
      "        [ 0.3934],\n",
      "        [-1.8541],\n",
      "        [ 0.4614],\n",
      "        [ 0.2316],\n",
      "        [-0.8783],\n",
      "        [ 0.4064],\n",
      "        [-0.6521],\n",
      "        [-0.7988],\n",
      "        [ 0.3559],\n",
      "        [ 0.4037],\n",
      "        [ 1.0780],\n",
      "        [ 2.2621],\n",
      "        [ 0.9758],\n",
      "        [-0.6729],\n",
      "        [ 0.6896],\n",
      "        [-0.2085],\n",
      "        [-0.5761],\n",
      "        [-0.8101],\n",
      "        [ 0.0228],\n",
      "        [-1.3281],\n",
      "        [-1.2291],\n",
      "        [-0.4942],\n",
      "        [-0.4759],\n",
      "        [-0.0746],\n",
      "        [-1.4701],\n",
      "        [ 0.1043],\n",
      "        [ 2.9275],\n",
      "        [ 1.8400],\n",
      "        [-0.6355],\n",
      "        [ 0.0485],\n",
      "        [-0.9719],\n",
      "        [-0.2518],\n",
      "        [ 0.2189],\n",
      "        [-0.0392],\n",
      "        [ 0.8233],\n",
      "        [-0.4909],\n",
      "        [-1.0899],\n",
      "        [-0.9201],\n",
      "        [ 0.3952],\n",
      "        [ 0.8206],\n",
      "        [-0.2937],\n",
      "        [-1.5851],\n",
      "        [-1.7345],\n",
      "        [-0.2216],\n",
      "        [ 0.1250],\n",
      "        [ 0.9619],\n",
      "        [-0.5134],\n",
      "        [ 0.0620],\n",
      "        [ 1.3327],\n",
      "        [-1.4112],\n",
      "        [-0.6687],\n",
      "        [ 0.9890],\n",
      "        [ 0.9875],\n",
      "        [ 0.3570],\n",
      "        [ 0.0159],\n",
      "        [ 0.4761],\n",
      "        [-0.1769],\n",
      "        [-0.9714],\n",
      "        [-0.1955],\n",
      "        [-0.4162],\n",
      "        [ 0.1356],\n",
      "        [ 0.3041],\n",
      "        [ 1.4062],\n",
      "        [ 1.5633],\n",
      "        [ 2.0052],\n",
      "        [-0.1437],\n",
      "        [-1.3978],\n",
      "        [-1.1809],\n",
      "        [ 0.3202],\n",
      "        [ 1.0467],\n",
      "        [ 0.8193],\n",
      "        [ 0.0884],\n",
      "        [ 1.0905],\n",
      "        [ 1.3133],\n",
      "        [-0.8055],\n",
      "        [ 0.8036],\n",
      "        [-0.7617],\n",
      "        [-2.2044],\n",
      "        [-1.4519],\n",
      "        [-0.8137],\n",
      "        [ 0.4150],\n",
      "        [-0.9555],\n",
      "        [ 0.4181],\n",
      "        [ 1.0902],\n",
      "        [ 1.1025],\n",
      "        [ 0.0116],\n",
      "        [-0.6234],\n",
      "        [ 0.4284],\n",
      "        [ 0.6339],\n",
      "        [-0.4250],\n",
      "        [ 1.5732],\n",
      "        [-0.6138],\n",
      "        [-0.5326],\n",
      "        [ 0.7680],\n",
      "        [ 0.2594],\n",
      "        [ 1.1065],\n",
      "        [-0.5940],\n",
      "        [ 0.3153],\n",
      "        [-0.3026],\n",
      "        [-1.8003],\n",
      "        [-0.5082],\n",
      "        [-0.9119],\n",
      "        [-1.6576],\n",
      "        [-1.2798],\n",
      "        [ 1.0336],\n",
      "        [ 1.5371],\n",
      "        [-0.7845],\n",
      "        [-0.1590],\n",
      "        [-0.3635],\n",
      "        [-1.0296],\n",
      "        [-0.7552],\n",
      "        [-0.2716],\n",
      "        [ 0.9570],\n",
      "        [-1.0281],\n",
      "        [ 0.0933],\n",
      "        [ 0.0710],\n",
      "        [-0.0048],\n",
      "        [-0.8125],\n",
      "        [-0.5611],\n",
      "        [ 0.0455],\n",
      "        [ 1.2462],\n",
      "        [ 0.7459],\n",
      "        [ 2.0458],\n",
      "        [-0.2101],\n",
      "        [ 0.0266],\n",
      "        [ 0.1611],\n",
      "        [ 0.5301],\n",
      "        [ 0.1608],\n",
      "        [-1.8435],\n",
      "        [-0.5004],\n",
      "        [-0.2150],\n",
      "        [-0.7402],\n",
      "        [-0.1645],\n",
      "        [-0.7629],\n",
      "        [ 0.0572],\n",
      "        [ 0.1866],\n",
      "        [-1.2048],\n",
      "        [-0.1744],\n",
      "        [ 1.0331],\n",
      "        [-1.0247],\n",
      "        [-0.1486],\n",
      "        [ 0.9350],\n",
      "        [-0.7953],\n",
      "        [ 0.6644],\n",
      "        [ 0.1819],\n",
      "        [-0.5604],\n",
      "        [-1.9933],\n",
      "        [ 0.6669],\n",
      "        [-1.3893],\n",
      "        [-1.0305],\n",
      "        [ 1.0941],\n",
      "        [-0.0119],\n",
      "        [ 1.4137],\n",
      "        [-0.5696],\n",
      "        [-0.0208],\n",
      "        [ 0.3490],\n",
      "        [ 1.2483],\n",
      "        [ 0.9272],\n",
      "        [-1.4759],\n",
      "        [-0.5865],\n",
      "        [ 0.1076],\n",
      "        [-0.1662],\n",
      "        [ 0.1274],\n",
      "        [-0.0230],\n",
      "        [ 0.8746],\n",
      "        [-0.9039],\n",
      "        [-0.7177],\n",
      "        [ 0.0524],\n",
      "        [ 0.2798]]), tensor([[0.0831],\n",
      "        [0.8578],\n",
      "        [0.6523],\n",
      "        [0.0698],\n",
      "        [0.1699],\n",
      "        [0.5403],\n",
      "        [0.5336],\n",
      "        [0.2160],\n",
      "        [0.7646],\n",
      "        [0.5498],\n",
      "        [0.4765],\n",
      "        [0.7651],\n",
      "        [0.5520],\n",
      "        [0.6280],\n",
      "        [0.4669],\n",
      "        [0.5472],\n",
      "        [0.3352],\n",
      "        [0.1701],\n",
      "        [0.6070],\n",
      "        [0.8571],\n",
      "        [0.8462],\n",
      "        [0.1533],\n",
      "        [0.3858],\n",
      "        [0.7171],\n",
      "        [0.7822],\n",
      "        [0.5208],\n",
      "        [0.3221],\n",
      "        [0.6747],\n",
      "        [0.4327],\n",
      "        [0.8596],\n",
      "        [0.2720],\n",
      "        [0.2851],\n",
      "        [0.1062],\n",
      "        [0.8202],\n",
      "        [0.5849],\n",
      "        [0.6459],\n",
      "        [0.1730],\n",
      "        [0.5521],\n",
      "        [0.6466],\n",
      "        [0.4200],\n",
      "        [0.6693],\n",
      "        [0.3689],\n",
      "        [0.0763],\n",
      "        [0.4836],\n",
      "        [0.4457],\n",
      "        [0.3890],\n",
      "        [0.6715],\n",
      "        [0.3454],\n",
      "        [0.3072],\n",
      "        [0.8673],\n",
      "        [0.0133],\n",
      "        [0.5349],\n",
      "        [0.1422],\n",
      "        [0.8031],\n",
      "        [0.7491],\n",
      "        [0.9396],\n",
      "        [0.3846],\n",
      "        [0.9359],\n",
      "        [0.2690],\n",
      "        [0.7673],\n",
      "        [0.7586],\n",
      "        [0.5593],\n",
      "        [0.3406],\n",
      "        [0.6577],\n",
      "        [0.9895],\n",
      "        [0.5268],\n",
      "        [0.4814],\n",
      "        [0.4700],\n",
      "        [0.5998],\n",
      "        [0.0670],\n",
      "        [0.3023],\n",
      "        [0.9134],\n",
      "        [0.2012],\n",
      "        [0.9017],\n",
      "        [0.6808],\n",
      "        [0.3671],\n",
      "        [0.4681],\n",
      "        [0.0671],\n",
      "        [0.1011],\n",
      "        [0.3009],\n",
      "        [0.9041],\n",
      "        [0.9528],\n",
      "        [0.8271],\n",
      "        [0.2684],\n",
      "        [0.9331],\n",
      "        [0.6282],\n",
      "        [0.4162],\n",
      "        [0.2847],\n",
      "        [0.8110],\n",
      "        [0.6784],\n",
      "        [0.6420],\n",
      "        [0.9881],\n",
      "        [0.7432],\n",
      "        [0.8343],\n",
      "        [0.6582],\n",
      "        [0.2323],\n",
      "        [0.9563],\n",
      "        [0.5965],\n",
      "        [0.5308],\n",
      "        [0.6014],\n",
      "        [0.4738],\n",
      "        [0.5416],\n",
      "        [0.8426],\n",
      "        [0.0577],\n",
      "        [0.4707],\n",
      "        [0.5233],\n",
      "        [0.4194],\n",
      "        [0.5389],\n",
      "        [0.4274],\n",
      "        [0.4677],\n",
      "        [0.6237],\n",
      "        [0.5223],\n",
      "        [0.2979],\n",
      "        [0.5995],\n",
      "        [0.6531],\n",
      "        [0.7395],\n",
      "        [0.5942],\n",
      "        [0.5727],\n",
      "        [0.5291],\n",
      "        [0.4258],\n",
      "        [0.1926],\n",
      "        [0.1891],\n",
      "        [0.9296],\n",
      "        [0.7808],\n",
      "        [0.2195],\n",
      "        [0.3416],\n",
      "        [0.6395],\n",
      "        [0.9345],\n",
      "        [0.4569],\n",
      "        [0.6675],\n",
      "        [0.2232],\n",
      "        [0.3080],\n",
      "        [0.1828],\n",
      "        [0.9091],\n",
      "        [0.0981],\n",
      "        [0.0533],\n",
      "        [0.7658],\n",
      "        [0.6356],\n",
      "        [0.9237],\n",
      "        [0.6705],\n",
      "        [0.7847],\n",
      "        [0.6533],\n",
      "        [0.3860],\n",
      "        [0.0349],\n",
      "        [0.5832],\n",
      "        [0.7848],\n",
      "        [0.1319],\n",
      "        [0.8625],\n",
      "        [0.0959],\n",
      "        [0.9549],\n",
      "        [0.3523],\n",
      "        [0.8413],\n",
      "        [0.8925],\n",
      "        [0.5321],\n",
      "        [0.3911],\n",
      "        [0.9035],\n",
      "        [0.6874],\n",
      "        [0.1355],\n",
      "        [0.8845],\n",
      "        [0.6086],\n",
      "        [0.4797],\n",
      "        [0.1852],\n",
      "        [0.2727],\n",
      "        [0.3218],\n",
      "        [0.1505],\n",
      "        [0.0224],\n",
      "        [0.2993],\n",
      "        [0.9386],\n",
      "        [0.0812],\n",
      "        [0.6887],\n",
      "        [0.6250],\n",
      "        [0.3104],\n",
      "        [0.8518],\n",
      "        [0.1354],\n",
      "        [0.1834],\n",
      "        [0.8501],\n",
      "        [0.7333],\n",
      "        [0.5678],\n",
      "        [0.3283],\n",
      "        [0.1532],\n",
      "        [0.9407],\n",
      "        [0.0263],\n",
      "        [0.7230],\n",
      "        [0.0771],\n",
      "        [0.4070],\n",
      "        [0.2360],\n",
      "        [0.1534],\n",
      "        [0.7730],\n",
      "        [0.6057],\n",
      "        [0.9924],\n",
      "        [0.8901],\n",
      "        [0.1228],\n",
      "        [0.0090],\n",
      "        [0.7195],\n",
      "        [0.1538],\n",
      "        [0.6628],\n",
      "        [0.1088],\n",
      "        [0.8851],\n",
      "        [0.6083],\n",
      "        [0.1651]])]\n",
      "Batch: 1, Data: [tensor([[-7.0922e-01],\n",
      "        [ 9.9254e-02],\n",
      "        [-9.5859e-01],\n",
      "        [ 3.1244e-02],\n",
      "        [ 7.5310e-01],\n",
      "        [ 4.2233e-01],\n",
      "        [ 6.2384e-01],\n",
      "        [-4.5710e-01],\n",
      "        [ 2.2143e-01],\n",
      "        [-5.2088e-01],\n",
      "        [ 6.2455e-02],\n",
      "        [ 2.4922e+00],\n",
      "        [ 7.2127e-02],\n",
      "        [ 9.3175e-01],\n",
      "        [-1.5405e+00],\n",
      "        [-1.7856e-01],\n",
      "        [-7.8956e-01],\n",
      "        [-5.7200e-01],\n",
      "        [ 7.2550e-01],\n",
      "        [-7.2286e-01],\n",
      "        [-1.8367e-01],\n",
      "        [-7.2376e-01],\n",
      "        [ 1.2007e+00],\n",
      "        [ 5.4647e-01],\n",
      "        [ 1.7951e+00],\n",
      "        [ 6.3938e-01],\n",
      "        [ 4.5855e-01],\n",
      "        [ 1.5710e+00],\n",
      "        [ 1.5642e+00],\n",
      "        [-1.4094e+00],\n",
      "        [-1.6687e+00],\n",
      "        [ 7.8088e-01],\n",
      "        [ 3.4006e-01],\n",
      "        [-3.5711e-02],\n",
      "        [-3.0101e-01],\n",
      "        [ 1.3425e-02],\n",
      "        [-1.7584e-01],\n",
      "        [-2.1742e+00],\n",
      "        [-5.2471e-02],\n",
      "        [ 1.5057e-01],\n",
      "        [ 1.6481e+00],\n",
      "        [ 1.2299e+00],\n",
      "        [-2.4023e+00],\n",
      "        [-9.5888e-01],\n",
      "        [ 1.2234e+00],\n",
      "        [-8.9251e-01],\n",
      "        [ 4.4541e-01],\n",
      "        [-1.2872e+00],\n",
      "        [ 1.4006e-01],\n",
      "        [ 7.7942e-01],\n",
      "        [-1.5285e+00],\n",
      "        [ 1.2532e+00],\n",
      "        [ 3.2934e-01],\n",
      "        [ 4.1780e-01],\n",
      "        [ 6.5143e-01],\n",
      "        [-1.4662e+00],\n",
      "        [-9.8538e-01],\n",
      "        [-6.5661e-01],\n",
      "        [-3.2516e-01],\n",
      "        [-1.0036e+00],\n",
      "        [ 4.9066e-01],\n",
      "        [-1.3146e+00],\n",
      "        [-1.1428e+00],\n",
      "        [ 7.4636e-01],\n",
      "        [-1.0202e+00],\n",
      "        [ 1.8224e+00],\n",
      "        [ 8.8480e-01],\n",
      "        [ 2.4162e-01],\n",
      "        [-6.0617e-02],\n",
      "        [-1.2750e+00],\n",
      "        [-6.2217e-01],\n",
      "        [ 1.4162e-01],\n",
      "        [-1.2900e+00],\n",
      "        [ 1.6799e+00],\n",
      "        [-8.5748e-01],\n",
      "        [-1.0070e+00],\n",
      "        [-1.6353e+00],\n",
      "        [ 2.2975e+00],\n",
      "        [-8.2856e-01],\n",
      "        [ 3.5211e-01],\n",
      "        [-1.4296e+00],\n",
      "        [ 2.6599e-01],\n",
      "        [-1.0356e+00],\n",
      "        [ 5.5070e-01],\n",
      "        [-2.5454e-01],\n",
      "        [ 1.2434e-01],\n",
      "        [-2.0786e-01],\n",
      "        [-5.3222e-01],\n",
      "        [-4.1619e-01],\n",
      "        [-4.9497e-01],\n",
      "        [ 2.1432e+00],\n",
      "        [ 1.1969e+00],\n",
      "        [-2.5547e-01],\n",
      "        [-2.5319e-01],\n",
      "        [ 3.2640e-01],\n",
      "        [-9.9004e-01],\n",
      "        [ 7.2530e-01],\n",
      "        [-1.8460e-01],\n",
      "        [ 7.3769e-01],\n",
      "        [ 1.2397e+00],\n",
      "        [ 9.6453e-01],\n",
      "        [ 2.1990e-01],\n",
      "        [ 1.1388e+00],\n",
      "        [ 1.1928e+00],\n",
      "        [ 8.4978e-01],\n",
      "        [ 7.3754e-01],\n",
      "        [-1.7377e-03],\n",
      "        [ 7.5287e-01],\n",
      "        [-1.7304e+00],\n",
      "        [ 9.5270e-01],\n",
      "        [-3.5141e-01],\n",
      "        [-2.5330e+00],\n",
      "        [-1.2789e+00],\n",
      "        [ 2.9216e-01],\n",
      "        [ 2.3865e-01],\n",
      "        [ 4.9310e-01],\n",
      "        [-2.1901e+00],\n",
      "        [ 8.3799e-01],\n",
      "        [ 1.1507e+00],\n",
      "        [ 5.2530e-02],\n",
      "        [-4.0140e-01],\n",
      "        [ 2.6253e-01],\n",
      "        [ 4.9894e-01],\n",
      "        [-1.6890e+00],\n",
      "        [-7.4562e-01],\n",
      "        [ 4.6138e-01],\n",
      "        [ 3.4551e-01],\n",
      "        [ 7.2859e-01],\n",
      "        [-2.7932e-01],\n",
      "        [ 4.6972e-01],\n",
      "        [ 2.4485e-01],\n",
      "        [-1.9439e-01],\n",
      "        [ 1.7938e-01],\n",
      "        [-2.4127e+00],\n",
      "        [ 5.0905e-01],\n",
      "        [-7.2180e-02],\n",
      "        [ 4.1117e-02],\n",
      "        [-1.0698e+00],\n",
      "        [-3.5859e-01],\n",
      "        [ 3.5802e-01],\n",
      "        [ 2.2981e-01],\n",
      "        [ 1.4297e-01],\n",
      "        [ 6.6423e-01],\n",
      "        [ 3.2154e-01],\n",
      "        [ 2.0431e-01],\n",
      "        [ 4.6061e-01],\n",
      "        [ 1.1428e+00],\n",
      "        [-8.5824e-01],\n",
      "        [-1.0220e+00],\n",
      "        [ 2.2039e+00],\n",
      "        [-6.4270e-01],\n",
      "        [ 1.0697e+00],\n",
      "        [ 1.0491e+00],\n",
      "        [ 1.0726e+00],\n",
      "        [-1.0791e-01],\n",
      "        [ 2.2050e-01],\n",
      "        [ 2.0672e-01],\n",
      "        [ 1.1230e+00],\n",
      "        [-2.1386e-01],\n",
      "        [ 6.4642e-01],\n",
      "        [-3.0951e-01],\n",
      "        [-4.3992e-01],\n",
      "        [ 6.4559e-02],\n",
      "        [ 5.7968e-01],\n",
      "        [-8.0042e-01],\n",
      "        [ 8.2359e-01],\n",
      "        [ 4.3431e-04],\n",
      "        [-1.0413e+00],\n",
      "        [-2.8105e-01],\n",
      "        [-1.4191e-02],\n",
      "        [-1.3930e+00],\n",
      "        [-8.0526e-02],\n",
      "        [-9.0658e-01],\n",
      "        [-1.3096e-01],\n",
      "        [ 3.8621e-01],\n",
      "        [-1.3895e-01],\n",
      "        [-3.8763e-01],\n",
      "        [ 5.6160e-01],\n",
      "        [-1.0747e+00],\n",
      "        [ 6.4500e-01],\n",
      "        [-2.8626e-02],\n",
      "        [-4.6459e-01],\n",
      "        [ 8.0128e-01],\n",
      "        [ 1.6822e-01],\n",
      "        [-5.2063e-01],\n",
      "        [-1.2734e+00],\n",
      "        [-4.6662e-01],\n",
      "        [ 1.3326e+00],\n",
      "        [-5.1129e-02],\n",
      "        [-7.1129e-01],\n",
      "        [ 1.1859e+00],\n",
      "        [-2.3067e+00],\n",
      "        [ 1.7669e-01],\n",
      "        [ 8.3189e-01],\n",
      "        [-1.0967e+00],\n",
      "        [-6.0299e-01],\n",
      "        [ 2.6405e+00],\n",
      "        [ 2.0124e+00],\n",
      "        [-5.7182e-01],\n",
      "        [-1.3465e+00]]), tensor([[4.6690e-01],\n",
      "        [5.5755e-01],\n",
      "        [3.1207e-01],\n",
      "        [5.5948e-01],\n",
      "        [4.6495e-02],\n",
      "        [3.0995e-01],\n",
      "        [3.3181e-01],\n",
      "        [2.8082e-01],\n",
      "        [4.6483e-01],\n",
      "        [7.7596e-01],\n",
      "        [4.0069e-01],\n",
      "        [2.4964e-01],\n",
      "        [6.1914e-01],\n",
      "        [7.3956e-01],\n",
      "        [8.2225e-01],\n",
      "        [6.9132e-01],\n",
      "        [3.3294e-01],\n",
      "        [5.3617e-01],\n",
      "        [6.2448e-01],\n",
      "        [2.1540e-01],\n",
      "        [8.7382e-01],\n",
      "        [5.4134e-01],\n",
      "        [5.2243e-01],\n",
      "        [4.7261e-01],\n",
      "        [7.7032e-01],\n",
      "        [7.1923e-01],\n",
      "        [7.6382e-01],\n",
      "        [8.9454e-01],\n",
      "        [3.5527e-01],\n",
      "        [9.1826e-01],\n",
      "        [7.1630e-01],\n",
      "        [4.1234e-01],\n",
      "        [4.5434e-01],\n",
      "        [1.2894e-01],\n",
      "        [3.6794e-01],\n",
      "        [3.6832e-01],\n",
      "        [5.9542e-01],\n",
      "        [5.7621e-01],\n",
      "        [1.7028e-01],\n",
      "        [8.5863e-02],\n",
      "        [2.0059e-01],\n",
      "        [4.5517e-01],\n",
      "        [9.9949e-01],\n",
      "        [4.7509e-01],\n",
      "        [3.5727e-01],\n",
      "        [3.3274e-01],\n",
      "        [8.0559e-01],\n",
      "        [9.2898e-01],\n",
      "        [3.7819e-01],\n",
      "        [1.1160e-01],\n",
      "        [5.6664e-01],\n",
      "        [7.9180e-01],\n",
      "        [4.5444e-02],\n",
      "        [5.5145e-02],\n",
      "        [3.3672e-01],\n",
      "        [9.3948e-01],\n",
      "        [7.8013e-01],\n",
      "        [1.5564e-01],\n",
      "        [2.4455e-01],\n",
      "        [4.5080e-01],\n",
      "        [6.9965e-01],\n",
      "        [9.0821e-01],\n",
      "        [4.1939e-01],\n",
      "        [2.8134e-01],\n",
      "        [5.6103e-01],\n",
      "        [5.9998e-01],\n",
      "        [2.7675e-01],\n",
      "        [4.3584e-01],\n",
      "        [2.9164e-01],\n",
      "        [1.6967e-01],\n",
      "        [3.2952e-01],\n",
      "        [8.0060e-02],\n",
      "        [3.3948e-01],\n",
      "        [6.9535e-01],\n",
      "        [1.2291e-01],\n",
      "        [1.0499e-01],\n",
      "        [7.8685e-01],\n",
      "        [4.2648e-01],\n",
      "        [4.6011e-01],\n",
      "        [8.4734e-01],\n",
      "        [8.3676e-01],\n",
      "        [4.6111e-03],\n",
      "        [5.1741e-01],\n",
      "        [1.9580e-04],\n",
      "        [5.9220e-01],\n",
      "        [6.6501e-01],\n",
      "        [9.7381e-01],\n",
      "        [4.8404e-01],\n",
      "        [7.9521e-01],\n",
      "        [1.5184e-01],\n",
      "        [2.8958e-01],\n",
      "        [6.9611e-01],\n",
      "        [7.9638e-01],\n",
      "        [8.8040e-01],\n",
      "        [3.3798e-01],\n",
      "        [7.6405e-01],\n",
      "        [2.7904e-02],\n",
      "        [4.3813e-01],\n",
      "        [5.5797e-01],\n",
      "        [5.2353e-01],\n",
      "        [3.9877e-01],\n",
      "        [1.8372e-01],\n",
      "        [2.7765e-01],\n",
      "        [5.7163e-02],\n",
      "        [4.6416e-01],\n",
      "        [3.6487e-01],\n",
      "        [2.2969e-01],\n",
      "        [6.4938e-01],\n",
      "        [7.6718e-01],\n",
      "        [3.2007e-02],\n",
      "        [7.6002e-01],\n",
      "        [1.9373e-01],\n",
      "        [2.9523e-01],\n",
      "        [7.2142e-01],\n",
      "        [1.9400e-01],\n",
      "        [4.7392e-01],\n",
      "        [4.8516e-01],\n",
      "        [2.4653e-01],\n",
      "        [5.7901e-01],\n",
      "        [1.8816e-01],\n",
      "        [1.8590e-02],\n",
      "        [4.3092e-01],\n",
      "        [7.4257e-01],\n",
      "        [8.8354e-01],\n",
      "        [4.5757e-01],\n",
      "        [6.0272e-02],\n",
      "        [2.5665e-03],\n",
      "        [7.8258e-01],\n",
      "        [6.2752e-01],\n",
      "        [9.0374e-01],\n",
      "        [4.2521e-01],\n",
      "        [9.9044e-01],\n",
      "        [1.6638e-01],\n",
      "        [7.4927e-01],\n",
      "        [3.7370e-01],\n",
      "        [9.7438e-01],\n",
      "        [3.0930e-01],\n",
      "        [4.4295e-01],\n",
      "        [6.6517e-01],\n",
      "        [7.4188e-01],\n",
      "        [8.9937e-01],\n",
      "        [4.0454e-01],\n",
      "        [8.9830e-01],\n",
      "        [6.9726e-02],\n",
      "        [4.3310e-01],\n",
      "        [8.5687e-01],\n",
      "        [5.8396e-01],\n",
      "        [8.1884e-01],\n",
      "        [9.3343e-01],\n",
      "        [5.9670e-01],\n",
      "        [4.1999e-01],\n",
      "        [8.0588e-01],\n",
      "        [6.9858e-01],\n",
      "        [4.3472e-01],\n",
      "        [7.3786e-02],\n",
      "        [2.0147e-01],\n",
      "        [5.7744e-01],\n",
      "        [3.5976e-02],\n",
      "        [1.6112e-01],\n",
      "        [9.7214e-01],\n",
      "        [6.5575e-02],\n",
      "        [4.4164e-01],\n",
      "        [5.3838e-01],\n",
      "        [8.1028e-03],\n",
      "        [3.8284e-01],\n",
      "        [5.5462e-02],\n",
      "        [8.9700e-01],\n",
      "        [8.6764e-01],\n",
      "        [3.3404e-01],\n",
      "        [9.8727e-01],\n",
      "        [5.3197e-01],\n",
      "        [9.3975e-01],\n",
      "        [3.9549e-01],\n",
      "        [2.2098e-01],\n",
      "        [9.6169e-01],\n",
      "        [1.5196e-01],\n",
      "        [3.0192e-01],\n",
      "        [2.1983e-01],\n",
      "        [7.8008e-01],\n",
      "        [5.2704e-01],\n",
      "        [6.2430e-01],\n",
      "        [9.6907e-01],\n",
      "        [5.1647e-01],\n",
      "        [4.1036e-01],\n",
      "        [6.4631e-01],\n",
      "        [9.9551e-01],\n",
      "        [7.0354e-01],\n",
      "        [4.5113e-01],\n",
      "        [7.2285e-01],\n",
      "        [8.1936e-01],\n",
      "        [4.1783e-01],\n",
      "        [4.2875e-01],\n",
      "        [9.9355e-01],\n",
      "        [5.9827e-01],\n",
      "        [8.3730e-01],\n",
      "        [8.8797e-01],\n",
      "        [4.5175e-01],\n",
      "        [5.1667e-01],\n",
      "        [4.1087e-01],\n",
      "        [1.9401e-01]])]\n",
      "Batch: 2, Data: [tensor([[-7.9371e-01],\n",
      "        [ 6.1524e-01],\n",
      "        [-1.3507e+00],\n",
      "        [-2.0761e-03],\n",
      "        [ 6.8096e-01],\n",
      "        [ 7.6459e-01],\n",
      "        [-5.2024e-01],\n",
      "        [ 6.7145e-01],\n",
      "        [-6.0580e-02],\n",
      "        [ 1.3722e-01],\n",
      "        [ 5.8093e-01],\n",
      "        [ 1.0942e+00],\n",
      "        [-2.3252e-01],\n",
      "        [-2.4445e+00],\n",
      "        [-7.3356e-01],\n",
      "        [ 3.6593e-01],\n",
      "        [-8.7257e-01],\n",
      "        [-4.6821e-01],\n",
      "        [ 8.7699e-01],\n",
      "        [ 3.1428e-01],\n",
      "        [-6.4908e-01],\n",
      "        [-5.6048e-01],\n",
      "        [ 1.9141e-01],\n",
      "        [-7.7888e-01],\n",
      "        [ 1.0772e+00],\n",
      "        [ 4.2606e-01],\n",
      "        [ 5.0914e-01],\n",
      "        [-6.8956e-01],\n",
      "        [ 8.8059e-01],\n",
      "        [-4.0330e-01],\n",
      "        [ 7.4937e-01],\n",
      "        [-7.1316e-02],\n",
      "        [-8.7380e-01],\n",
      "        [ 5.7152e-01],\n",
      "        [ 5.0916e-01],\n",
      "        [ 8.1218e-02],\n",
      "        [-7.6049e-01],\n",
      "        [ 1.7072e-01],\n",
      "        [ 7.9611e-01],\n",
      "        [ 9.7939e-02],\n",
      "        [-8.1609e-01],\n",
      "        [ 1.3403e+00],\n",
      "        [ 2.2656e-01],\n",
      "        [ 4.0763e-01],\n",
      "        [ 7.2496e-01],\n",
      "        [-1.0205e+00],\n",
      "        [ 1.5209e+00],\n",
      "        [-1.3685e+00],\n",
      "        [-4.1316e-01],\n",
      "        [-3.2198e-01],\n",
      "        [-2.9420e-01],\n",
      "        [-8.9187e-01],\n",
      "        [-4.7557e-01],\n",
      "        [-7.6083e-01],\n",
      "        [ 4.0985e-01],\n",
      "        [ 2.1690e+00],\n",
      "        [ 4.6000e-01],\n",
      "        [ 6.6924e-01],\n",
      "        [ 9.7366e-01],\n",
      "        [-1.7172e+00],\n",
      "        [ 5.4715e-01],\n",
      "        [ 1.3298e-01],\n",
      "        [ 2.6319e-01],\n",
      "        [ 4.0087e-01],\n",
      "        [ 1.1411e-02],\n",
      "        [-8.1590e-01],\n",
      "        [-2.5463e+00],\n",
      "        [-1.1446e+00],\n",
      "        [-7.3907e-01],\n",
      "        [-1.0017e-01],\n",
      "        [-1.2723e+00],\n",
      "        [-8.0959e-02],\n",
      "        [-6.4121e-01],\n",
      "        [-7.6984e-01],\n",
      "        [ 6.9373e-01],\n",
      "        [-5.0916e-01],\n",
      "        [-1.0732e+00],\n",
      "        [-1.0052e+00],\n",
      "        [ 5.1818e-01],\n",
      "        [-7.9275e-01],\n",
      "        [-6.5285e-01],\n",
      "        [-4.5228e-01],\n",
      "        [ 5.0745e-01],\n",
      "        [ 1.0116e+00],\n",
      "        [ 1.5424e+00],\n",
      "        [-5.9654e-01],\n",
      "        [-1.0391e+00],\n",
      "        [-5.1024e-01],\n",
      "        [ 9.9855e-01],\n",
      "        [-3.5724e-01],\n",
      "        [ 5.9286e-01],\n",
      "        [ 7.7338e-01],\n",
      "        [-1.1827e+00],\n",
      "        [-2.1331e-02],\n",
      "        [-1.1764e+00],\n",
      "        [-1.8802e+00],\n",
      "        [ 2.4900e-02],\n",
      "        [-1.9855e-01],\n",
      "        [-4.3442e-03],\n",
      "        [ 2.2639e-01],\n",
      "        [-8.4547e-01],\n",
      "        [ 9.3276e-01],\n",
      "        [ 2.0935e-01],\n",
      "        [-6.7856e-01],\n",
      "        [ 4.5737e-01],\n",
      "        [-3.5764e-01],\n",
      "        [-8.4697e-01],\n",
      "        [ 1.2591e+00],\n",
      "        [ 7.1152e-01],\n",
      "        [-3.5315e-01],\n",
      "        [ 1.3707e+00],\n",
      "        [ 1.6663e+00],\n",
      "        [-1.8390e+00],\n",
      "        [ 1.2690e-01],\n",
      "        [-1.4068e+00],\n",
      "        [ 2.2839e-01],\n",
      "        [ 1.9441e-01],\n",
      "        [-1.1019e+00],\n",
      "        [-7.1181e-01],\n",
      "        [ 9.4534e-01],\n",
      "        [ 1.4272e+00],\n",
      "        [-1.3247e+00],\n",
      "        [-1.0027e+00],\n",
      "        [ 7.1638e-01],\n",
      "        [-1.1039e+00],\n",
      "        [-1.8383e+00],\n",
      "        [-9.0218e-01],\n",
      "        [ 7.7231e-01],\n",
      "        [ 6.7177e-01],\n",
      "        [-1.6607e+00],\n",
      "        [-9.0148e-01],\n",
      "        [-1.0914e+00],\n",
      "        [ 1.7807e-01],\n",
      "        [-1.2202e-01],\n",
      "        [-4.2537e-01],\n",
      "        [-2.6807e-01],\n",
      "        [ 2.8681e-01],\n",
      "        [ 2.7855e-01],\n",
      "        [-2.1694e+00],\n",
      "        [-5.1034e-01],\n",
      "        [ 7.3901e-01],\n",
      "        [ 4.3627e-01],\n",
      "        [ 2.0426e-01],\n",
      "        [ 1.6958e+00],\n",
      "        [ 7.5025e-01],\n",
      "        [-6.1320e-01],\n",
      "        [-9.1754e-01],\n",
      "        [ 1.5316e+00],\n",
      "        [ 8.6619e-01],\n",
      "        [ 1.2539e-01],\n",
      "        [-1.5726e+00],\n",
      "        [ 1.7167e+00],\n",
      "        [-1.7810e+00],\n",
      "        [-4.3463e-01],\n",
      "        [-2.0484e+00],\n",
      "        [ 3.0386e-01],\n",
      "        [-3.5234e-01],\n",
      "        [-1.5298e+00],\n",
      "        [-1.8830e-01],\n",
      "        [ 1.1681e+00],\n",
      "        [ 1.4814e-01],\n",
      "        [ 4.5162e-02],\n",
      "        [ 6.0988e-01],\n",
      "        [ 6.8720e-01],\n",
      "        [-5.7541e-01],\n",
      "        [ 7.6146e-02],\n",
      "        [-1.3676e+00],\n",
      "        [ 1.2954e+00],\n",
      "        [-9.6345e-01],\n",
      "        [ 4.6911e-01],\n",
      "        [-2.9799e-01],\n",
      "        [ 1.1014e+00],\n",
      "        [-4.1365e-01],\n",
      "        [-7.9547e-01],\n",
      "        [ 2.1909e-01],\n",
      "        [ 1.1145e+00],\n",
      "        [-5.6825e-01],\n",
      "        [-1.8362e-01],\n",
      "        [-1.2994e+00],\n",
      "        [ 3.6501e-01],\n",
      "        [-5.6445e-01],\n",
      "        [ 1.7281e+00],\n",
      "        [-1.0971e+00],\n",
      "        [-8.2904e-01],\n",
      "        [-9.3585e-01],\n",
      "        [-1.7532e+00],\n",
      "        [-8.6720e-02],\n",
      "        [ 3.6819e-01],\n",
      "        [-7.2928e-01],\n",
      "        [-3.2666e-02],\n",
      "        [-4.9456e-01],\n",
      "        [ 1.1441e+00],\n",
      "        [ 1.9344e+00],\n",
      "        [ 1.0454e+00],\n",
      "        [-1.6735e+00],\n",
      "        [ 1.6416e+00],\n",
      "        [-5.5988e-01],\n",
      "        [-1.8049e-01],\n",
      "        [ 3.7954e-01],\n",
      "        [-1.3582e+00]]), tensor([[0.8153],\n",
      "        [0.7213],\n",
      "        [0.8097],\n",
      "        [0.9046],\n",
      "        [0.5426],\n",
      "        [0.6029],\n",
      "        [0.6880],\n",
      "        [0.5978],\n",
      "        [0.8080],\n",
      "        [0.5735],\n",
      "        [0.8551],\n",
      "        [0.8451],\n",
      "        [0.4178],\n",
      "        [0.0796],\n",
      "        [0.9965],\n",
      "        [0.2038],\n",
      "        [0.6394],\n",
      "        [0.9477],\n",
      "        [0.6016],\n",
      "        [0.8046],\n",
      "        [0.4603],\n",
      "        [0.0888],\n",
      "        [0.3849],\n",
      "        [0.6814],\n",
      "        [0.8972],\n",
      "        [0.3413],\n",
      "        [0.1810],\n",
      "        [0.6340],\n",
      "        [0.3396],\n",
      "        [0.1284],\n",
      "        [0.3130],\n",
      "        [0.1219],\n",
      "        [0.9769],\n",
      "        [0.6861],\n",
      "        [0.6731],\n",
      "        [0.8322],\n",
      "        [0.3663],\n",
      "        [0.3617],\n",
      "        [0.5048],\n",
      "        [0.2913],\n",
      "        [0.3142],\n",
      "        [0.6243],\n",
      "        [0.8108],\n",
      "        [0.8819],\n",
      "        [0.4812],\n",
      "        [0.0649],\n",
      "        [0.5740],\n",
      "        [0.3881],\n",
      "        [0.1388],\n",
      "        [0.8748],\n",
      "        [0.0692],\n",
      "        [0.5890],\n",
      "        [0.6681],\n",
      "        [0.8281],\n",
      "        [0.4079],\n",
      "        [0.8811],\n",
      "        [0.9944],\n",
      "        [0.2168],\n",
      "        [0.8196],\n",
      "        [0.9682],\n",
      "        [0.2454],\n",
      "        [0.2925],\n",
      "        [0.9534],\n",
      "        [0.6784],\n",
      "        [0.7156],\n",
      "        [0.1426],\n",
      "        [0.4514],\n",
      "        [0.7330],\n",
      "        [0.1238],\n",
      "        [0.9818],\n",
      "        [0.8138],\n",
      "        [0.8480],\n",
      "        [0.1462],\n",
      "        [0.7181],\n",
      "        [0.0310],\n",
      "        [0.4397],\n",
      "        [0.0561],\n",
      "        [0.8909],\n",
      "        [0.2834],\n",
      "        [0.1786],\n",
      "        [0.5887],\n",
      "        [0.4401],\n",
      "        [0.7893],\n",
      "        [0.1253],\n",
      "        [0.3516],\n",
      "        [0.6594],\n",
      "        [0.7701],\n",
      "        [0.5421],\n",
      "        [0.6753],\n",
      "        [0.2066],\n",
      "        [0.1254],\n",
      "        [0.4646],\n",
      "        [0.3024],\n",
      "        [0.2953],\n",
      "        [0.0242],\n",
      "        [0.0668],\n",
      "        [0.4729],\n",
      "        [0.2443],\n",
      "        [0.3294],\n",
      "        [0.9564],\n",
      "        [0.7039],\n",
      "        [0.7165],\n",
      "        [0.3581],\n",
      "        [0.7062],\n",
      "        [0.0482],\n",
      "        [0.3810],\n",
      "        [0.1491],\n",
      "        [0.4209],\n",
      "        [0.8820],\n",
      "        [0.1624],\n",
      "        [0.6704],\n",
      "        [0.7594],\n",
      "        [0.2454],\n",
      "        [0.6533],\n",
      "        [0.8893],\n",
      "        [0.2377],\n",
      "        [0.2507],\n",
      "        [0.7033],\n",
      "        [0.1510],\n",
      "        [0.9707],\n",
      "        [0.6673],\n",
      "        [0.4353],\n",
      "        [0.3238],\n",
      "        [0.6276],\n",
      "        [0.9977],\n",
      "        [0.8738],\n",
      "        [0.9443],\n",
      "        [0.1890],\n",
      "        [0.7274],\n",
      "        [0.7087],\n",
      "        [0.9181],\n",
      "        [0.5159],\n",
      "        [0.5393],\n",
      "        [0.4662],\n",
      "        [0.0371],\n",
      "        [0.5735],\n",
      "        [0.1245],\n",
      "        [0.7979],\n",
      "        [0.5172],\n",
      "        [0.5681],\n",
      "        [0.1986],\n",
      "        [0.2823],\n",
      "        [0.8217],\n",
      "        [0.6495],\n",
      "        [0.2964],\n",
      "        [0.0301],\n",
      "        [0.7699],\n",
      "        [0.8530],\n",
      "        [0.0019],\n",
      "        [0.1216],\n",
      "        [0.1298],\n",
      "        [0.7075],\n",
      "        [0.0310],\n",
      "        [0.1259],\n",
      "        [0.4740],\n",
      "        [0.5056],\n",
      "        [0.5125],\n",
      "        [0.7155],\n",
      "        [0.2511],\n",
      "        [0.7869],\n",
      "        [0.2377],\n",
      "        [0.0717],\n",
      "        [0.2973],\n",
      "        [0.1086],\n",
      "        [0.4184],\n",
      "        [0.7285],\n",
      "        [0.7617],\n",
      "        [0.7541],\n",
      "        [0.7364],\n",
      "        [0.4780],\n",
      "        [0.8226],\n",
      "        [0.5801],\n",
      "        [0.6579],\n",
      "        [0.8037],\n",
      "        [0.1114],\n",
      "        [0.8593],\n",
      "        [0.9365],\n",
      "        [0.3538],\n",
      "        [0.0293],\n",
      "        [0.0414],\n",
      "        [0.0054],\n",
      "        [0.9350],\n",
      "        [0.0210],\n",
      "        [0.4388],\n",
      "        [0.7759],\n",
      "        [0.4881],\n",
      "        [0.2989],\n",
      "        [0.1076],\n",
      "        [0.1040],\n",
      "        [0.4204],\n",
      "        [0.7266],\n",
      "        [0.4247],\n",
      "        [0.6190],\n",
      "        [0.6614],\n",
      "        [0.1597],\n",
      "        [0.6591],\n",
      "        [0.7703],\n",
      "        [0.8437],\n",
      "        [0.8382],\n",
      "        [0.5086]])]\n",
      "Batch: 3, Data: [tensor([[-0.2756],\n",
      "        [ 0.3747],\n",
      "        [ 0.1940],\n",
      "        [-0.3737],\n",
      "        [ 0.7187],\n",
      "        [ 0.4719],\n",
      "        [-0.3020],\n",
      "        [ 0.0270],\n",
      "        [ 0.6429],\n",
      "        [-1.4004],\n",
      "        [-0.4639],\n",
      "        [-1.2138],\n",
      "        [-0.9346],\n",
      "        [ 0.5632],\n",
      "        [-1.0888],\n",
      "        [-1.4884],\n",
      "        [ 0.9201],\n",
      "        [-1.1494],\n",
      "        [ 0.5654],\n",
      "        [-2.3818],\n",
      "        [ 1.6991],\n",
      "        [-0.0443],\n",
      "        [ 0.7959],\n",
      "        [ 0.1224],\n",
      "        [ 0.2677],\n",
      "        [ 0.6626],\n",
      "        [ 0.1893],\n",
      "        [-1.8710],\n",
      "        [-0.8127],\n",
      "        [ 0.0520],\n",
      "        [-0.2395],\n",
      "        [ 0.8346],\n",
      "        [ 0.8585],\n",
      "        [-0.9274],\n",
      "        [-0.2350],\n",
      "        [ 1.0005],\n",
      "        [ 0.6075],\n",
      "        [ 0.2183],\n",
      "        [ 0.3160],\n",
      "        [ 0.4652],\n",
      "        [ 0.2116],\n",
      "        [-1.5082],\n",
      "        [ 1.0421],\n",
      "        [-0.6198],\n",
      "        [-0.6508],\n",
      "        [-1.1513],\n",
      "        [ 1.2067],\n",
      "        [ 0.0498],\n",
      "        [ 0.4839],\n",
      "        [ 2.3931],\n",
      "        [ 0.4919],\n",
      "        [ 0.1065],\n",
      "        [ 0.4142],\n",
      "        [-0.9194],\n",
      "        [ 0.1289],\n",
      "        [-1.7726],\n",
      "        [-0.7321],\n",
      "        [-1.0288],\n",
      "        [-0.4858],\n",
      "        [ 0.5940],\n",
      "        [-1.6467],\n",
      "        [-0.6640],\n",
      "        [ 0.2875],\n",
      "        [ 0.0626],\n",
      "        [ 0.2100],\n",
      "        [ 0.1489],\n",
      "        [ 0.3233],\n",
      "        [-0.6402],\n",
      "        [ 1.6709],\n",
      "        [-0.7684],\n",
      "        [-2.4398],\n",
      "        [ 0.2954],\n",
      "        [ 0.7303],\n",
      "        [-0.1072],\n",
      "        [ 0.7539],\n",
      "        [ 0.8901],\n",
      "        [-0.4876],\n",
      "        [-0.5480],\n",
      "        [ 0.1190],\n",
      "        [ 1.4915],\n",
      "        [-0.5480],\n",
      "        [ 0.2649],\n",
      "        [ 2.2380],\n",
      "        [ 2.5348],\n",
      "        [ 0.6871],\n",
      "        [-0.1100],\n",
      "        [-1.9749],\n",
      "        [-0.4514],\n",
      "        [ 0.6634],\n",
      "        [ 0.3711],\n",
      "        [ 0.1546],\n",
      "        [ 0.4535],\n",
      "        [-1.0939],\n",
      "        [-1.0473],\n",
      "        [-0.5212],\n",
      "        [ 1.3776],\n",
      "        [-0.8301],\n",
      "        [-0.0596],\n",
      "        [ 0.3098],\n",
      "        [ 0.9436],\n",
      "        [ 0.5026],\n",
      "        [ 0.2188],\n",
      "        [ 0.0128],\n",
      "        [ 0.3873],\n",
      "        [ 2.4197],\n",
      "        [-0.4381],\n",
      "        [-0.1673],\n",
      "        [-0.0331],\n",
      "        [-0.4194],\n",
      "        [ 2.1563],\n",
      "        [-0.4968],\n",
      "        [ 1.4023],\n",
      "        [ 2.1889],\n",
      "        [-0.4106],\n",
      "        [-0.5138],\n",
      "        [ 1.6571],\n",
      "        [ 2.2175],\n",
      "        [-1.5239],\n",
      "        [ 0.5435],\n",
      "        [ 1.5501],\n",
      "        [-0.7798],\n",
      "        [-1.3261],\n",
      "        [-0.4003],\n",
      "        [-0.5939],\n",
      "        [-0.5449],\n",
      "        [ 2.0274],\n",
      "        [ 1.5077],\n",
      "        [ 0.7466],\n",
      "        [ 1.0865],\n",
      "        [ 0.9491],\n",
      "        [ 1.5321],\n",
      "        [-0.5214],\n",
      "        [-0.4158],\n",
      "        [ 0.0027],\n",
      "        [ 1.8147],\n",
      "        [ 0.3300],\n",
      "        [ 0.8978],\n",
      "        [-0.6353],\n",
      "        [ 0.3028],\n",
      "        [ 0.6842],\n",
      "        [ 0.6109],\n",
      "        [-0.0831],\n",
      "        [ 1.1482],\n",
      "        [ 0.6905],\n",
      "        [-1.4761],\n",
      "        [-0.0254],\n",
      "        [ 0.5810],\n",
      "        [-0.6250],\n",
      "        [-0.8834],\n",
      "        [ 0.2531],\n",
      "        [ 1.0108],\n",
      "        [ 0.2725],\n",
      "        [-0.1205],\n",
      "        [-2.2242],\n",
      "        [-0.4254],\n",
      "        [ 0.5524],\n",
      "        [ 1.4572],\n",
      "        [-2.1002],\n",
      "        [-1.6209],\n",
      "        [-0.3730],\n",
      "        [ 0.4656],\n",
      "        [ 0.2451],\n",
      "        [-0.8449],\n",
      "        [-0.1646],\n",
      "        [ 1.8923],\n",
      "        [ 0.4061],\n",
      "        [ 0.1741],\n",
      "        [-0.3854],\n",
      "        [-0.8919],\n",
      "        [-0.6289],\n",
      "        [ 1.8846],\n",
      "        [ 0.2840],\n",
      "        [ 1.0321],\n",
      "        [-1.6803],\n",
      "        [ 0.0269],\n",
      "        [ 0.2695],\n",
      "        [ 0.7611],\n",
      "        [-2.6423],\n",
      "        [-0.1920],\n",
      "        [-0.5223],\n",
      "        [-1.0617],\n",
      "        [ 0.8186],\n",
      "        [-0.8212],\n",
      "        [ 0.5839],\n",
      "        [-1.2433],\n",
      "        [ 0.9326],\n",
      "        [-2.1420],\n",
      "        [ 0.0666],\n",
      "        [-0.2743],\n",
      "        [ 0.7259],\n",
      "        [ 0.2463],\n",
      "        [ 0.6282],\n",
      "        [-0.4353],\n",
      "        [-2.4381],\n",
      "        [-1.2281],\n",
      "        [ 0.9999],\n",
      "        [ 0.9776],\n",
      "        [-0.2460],\n",
      "        [ 0.6929],\n",
      "        [ 1.2469]]), tensor([[0.7475],\n",
      "        [0.4204],\n",
      "        [0.1242],\n",
      "        [0.2476],\n",
      "        [0.8744],\n",
      "        [0.3701],\n",
      "        [0.3316],\n",
      "        [0.7258],\n",
      "        [0.8356],\n",
      "        [0.5036],\n",
      "        [0.6968],\n",
      "        [0.2765],\n",
      "        [0.3683],\n",
      "        [0.7646],\n",
      "        [0.9369],\n",
      "        [0.5697],\n",
      "        [0.8458],\n",
      "        [0.8828],\n",
      "        [0.4333],\n",
      "        [0.7443],\n",
      "        [0.3334],\n",
      "        [0.1642],\n",
      "        [0.6668],\n",
      "        [0.5759],\n",
      "        [0.6689],\n",
      "        [0.9573],\n",
      "        [0.9990],\n",
      "        [0.7155],\n",
      "        [0.2990],\n",
      "        [0.2341],\n",
      "        [0.9682],\n",
      "        [0.2373],\n",
      "        [0.7956],\n",
      "        [0.4211],\n",
      "        [0.9453],\n",
      "        [0.6476],\n",
      "        [0.6679],\n",
      "        [0.0160],\n",
      "        [0.5988],\n",
      "        [0.2274],\n",
      "        [0.7777],\n",
      "        [0.0816],\n",
      "        [0.6839],\n",
      "        [0.8690],\n",
      "        [0.0883],\n",
      "        [0.8012],\n",
      "        [0.3004],\n",
      "        [0.3267],\n",
      "        [0.7782],\n",
      "        [0.5077],\n",
      "        [0.0792],\n",
      "        [0.9617],\n",
      "        [0.8157],\n",
      "        [0.7822],\n",
      "        [0.2387],\n",
      "        [0.3671],\n",
      "        [0.1573],\n",
      "        [0.3087],\n",
      "        [0.1152],\n",
      "        [0.3621],\n",
      "        [0.0569],\n",
      "        [0.3877],\n",
      "        [0.8615],\n",
      "        [0.5218],\n",
      "        [0.1459],\n",
      "        [0.9055],\n",
      "        [0.5460],\n",
      "        [0.4303],\n",
      "        [0.4435],\n",
      "        [0.0452],\n",
      "        [0.8830],\n",
      "        [0.2042],\n",
      "        [0.7879],\n",
      "        [0.5749],\n",
      "        [0.2109],\n",
      "        [0.9852],\n",
      "        [0.2032],\n",
      "        [0.1359],\n",
      "        [0.5534],\n",
      "        [0.7737],\n",
      "        [0.3936],\n",
      "        [0.4588],\n",
      "        [0.5307],\n",
      "        [0.1636],\n",
      "        [0.0447],\n",
      "        [0.0892],\n",
      "        [0.8609],\n",
      "        [0.2331],\n",
      "        [0.5042],\n",
      "        [0.6783],\n",
      "        [0.4272],\n",
      "        [0.9537],\n",
      "        [0.0117],\n",
      "        [0.7084],\n",
      "        [0.8277],\n",
      "        [0.4658],\n",
      "        [0.6913],\n",
      "        [0.6355],\n",
      "        [0.3511],\n",
      "        [0.9492],\n",
      "        [0.7094],\n",
      "        [0.6809],\n",
      "        [0.9051],\n",
      "        [0.7820],\n",
      "        [0.2482],\n",
      "        [0.8885],\n",
      "        [0.6476],\n",
      "        [0.1280],\n",
      "        [0.2158],\n",
      "        [0.8807],\n",
      "        [0.1011],\n",
      "        [0.0609],\n",
      "        [0.2358],\n",
      "        [0.2705],\n",
      "        [0.4156],\n",
      "        [0.5358],\n",
      "        [0.3316],\n",
      "        [0.4130],\n",
      "        [0.7783],\n",
      "        [0.2451],\n",
      "        [0.0855],\n",
      "        [0.0286],\n",
      "        [0.0765],\n",
      "        [0.0076],\n",
      "        [0.4939],\n",
      "        [0.1839],\n",
      "        [0.0240],\n",
      "        [0.9711],\n",
      "        [0.7818],\n",
      "        [0.3397],\n",
      "        [0.3496],\n",
      "        [0.4235],\n",
      "        [0.4223],\n",
      "        [0.4538],\n",
      "        [0.4443],\n",
      "        [0.1517],\n",
      "        [0.7122],\n",
      "        [0.2923],\n",
      "        [0.4184],\n",
      "        [0.9489],\n",
      "        [0.4524],\n",
      "        [0.5305],\n",
      "        [0.1141],\n",
      "        [0.2467],\n",
      "        [0.0017],\n",
      "        [0.0918],\n",
      "        [0.5675],\n",
      "        [0.5948],\n",
      "        [0.1787],\n",
      "        [0.5288],\n",
      "        [0.8193],\n",
      "        [0.3666],\n",
      "        [0.7969],\n",
      "        [0.0624],\n",
      "        [0.1232],\n",
      "        [0.9928],\n",
      "        [0.9710],\n",
      "        [0.9473],\n",
      "        [0.9344],\n",
      "        [0.8970],\n",
      "        [0.0315],\n",
      "        [0.9187],\n",
      "        [0.2642],\n",
      "        [0.5370],\n",
      "        [0.8196],\n",
      "        [0.1505],\n",
      "        [0.3392],\n",
      "        [0.5167],\n",
      "        [0.0776],\n",
      "        [0.0680],\n",
      "        [0.8727],\n",
      "        [0.6489],\n",
      "        [0.7682],\n",
      "        [0.6873],\n",
      "        [0.8961],\n",
      "        [0.6974],\n",
      "        [0.1059],\n",
      "        [0.8615],\n",
      "        [0.8043],\n",
      "        [0.8070],\n",
      "        [0.0850],\n",
      "        [0.6656],\n",
      "        [0.1989],\n",
      "        [0.0750],\n",
      "        [0.3700],\n",
      "        [0.7147],\n",
      "        [0.8651],\n",
      "        [0.7446],\n",
      "        [0.8469],\n",
      "        [0.9206],\n",
      "        [0.0870],\n",
      "        [0.3421],\n",
      "        [0.7341],\n",
      "        [0.2250],\n",
      "        [0.6041],\n",
      "        [0.3239],\n",
      "        [0.3510],\n",
      "        [0.4521],\n",
      "        [0.3002],\n",
      "        [0.1136]])]\n",
      "Batch: 4, Data: [tensor([[ 4.1837e-01],\n",
      "        [-1.3707e+00],\n",
      "        [ 1.0248e+00],\n",
      "        [-3.0699e+00],\n",
      "        [-5.1449e-01],\n",
      "        [ 1.2380e+00],\n",
      "        [-1.0770e+00],\n",
      "        [-1.1128e+00],\n",
      "        [-1.6213e+00],\n",
      "        [ 9.4365e-02],\n",
      "        [-6.1763e-01],\n",
      "        [ 3.0622e-01],\n",
      "        [-5.2347e-02],\n",
      "        [-5.3183e-01],\n",
      "        [ 1.6725e-01],\n",
      "        [-1.0070e+00],\n",
      "        [-6.6626e-01],\n",
      "        [-3.0554e-02],\n",
      "        [ 2.4043e+00],\n",
      "        [-6.8257e-01],\n",
      "        [-5.6279e-01],\n",
      "        [-2.0343e-01],\n",
      "        [-1.7892e+00],\n",
      "        [ 1.6743e+00],\n",
      "        [ 6.8024e-02],\n",
      "        [-2.3227e-01],\n",
      "        [-4.7158e-01],\n",
      "        [-5.9582e-01],\n",
      "        [ 7.7856e-01],\n",
      "        [-1.4259e-01],\n",
      "        [ 2.2909e+00],\n",
      "        [ 1.3131e+00],\n",
      "        [ 8.5620e-01],\n",
      "        [-9.3971e-01],\n",
      "        [ 9.0482e-01],\n",
      "        [ 1.9919e+00],\n",
      "        [ 8.7868e-01],\n",
      "        [ 6.7151e-01],\n",
      "        [-1.1455e-02],\n",
      "        [ 5.0961e-02],\n",
      "        [-1.1581e+00],\n",
      "        [-1.0615e+00],\n",
      "        [ 3.5791e-01],\n",
      "        [-1.0272e+00],\n",
      "        [ 3.6555e-01],\n",
      "        [-6.4890e-01],\n",
      "        [-1.0635e-01],\n",
      "        [ 5.9432e-01],\n",
      "        [-4.3085e-01],\n",
      "        [-1.4360e+00],\n",
      "        [ 6.4193e-01],\n",
      "        [-1.2478e+00],\n",
      "        [ 1.4179e+00],\n",
      "        [-2.0746e-01],\n",
      "        [ 2.1146e+00],\n",
      "        [-1.3705e-01],\n",
      "        [ 3.0854e-01],\n",
      "        [ 1.1487e+00],\n",
      "        [-1.2305e+00],\n",
      "        [ 2.2996e+00],\n",
      "        [ 1.5094e+00],\n",
      "        [-5.0097e-01],\n",
      "        [-1.2495e+00],\n",
      "        [-4.4524e-01],\n",
      "        [ 1.2766e+00],\n",
      "        [-2.3406e-01],\n",
      "        [-5.6036e-01],\n",
      "        [-5.0566e-01],\n",
      "        [ 5.9869e-01],\n",
      "        [-3.6248e-01],\n",
      "        [-1.9110e+00],\n",
      "        [ 2.9341e+00],\n",
      "        [-1.5986e+00],\n",
      "        [-2.9759e-01],\n",
      "        [ 9.2265e-01],\n",
      "        [ 1.0338e-02],\n",
      "        [ 2.2178e+00],\n",
      "        [-6.6749e-01],\n",
      "        [ 5.6067e-01],\n",
      "        [-1.7109e+00],\n",
      "        [ 1.1298e+00],\n",
      "        [ 1.3681e+00],\n",
      "        [-8.5728e-01],\n",
      "        [ 1.6276e+00],\n",
      "        [-2.4902e-01],\n",
      "        [ 2.8064e-01],\n",
      "        [-8.9489e-01],\n",
      "        [-3.5593e-01],\n",
      "        [ 2.9938e-01],\n",
      "        [-1.2115e+00],\n",
      "        [-1.1997e+00],\n",
      "        [ 3.0798e-02],\n",
      "        [-1.0014e+00],\n",
      "        [-2.1569e+00],\n",
      "        [-1.7804e+00],\n",
      "        [-6.3223e-01],\n",
      "        [ 2.4881e-01],\n",
      "        [-1.5774e+00],\n",
      "        [ 8.0480e-01],\n",
      "        [ 8.1549e-01],\n",
      "        [ 1.5016e+00],\n",
      "        [-4.8556e-01],\n",
      "        [-9.1926e-01],\n",
      "        [ 4.0805e-01],\n",
      "        [-1.9168e+00],\n",
      "        [ 1.4069e+00],\n",
      "        [-1.9853e-01],\n",
      "        [-7.9680e-01],\n",
      "        [-9.0834e-02],\n",
      "        [ 7.2205e-01],\n",
      "        [-1.8112e+00],\n",
      "        [ 9.0075e-01],\n",
      "        [-1.0933e+00],\n",
      "        [-5.5081e-01],\n",
      "        [-4.9262e-01],\n",
      "        [ 1.4756e+00],\n",
      "        [ 3.7774e-02],\n",
      "        [ 1.5447e+00],\n",
      "        [ 1.3406e+00],\n",
      "        [ 9.3409e-01],\n",
      "        [-9.1817e-01],\n",
      "        [ 4.8371e-01],\n",
      "        [ 1.3908e+00],\n",
      "        [-1.5228e+00],\n",
      "        [ 9.2557e-01],\n",
      "        [ 3.0012e-01],\n",
      "        [-1.2916e+00],\n",
      "        [-7.6303e-01],\n",
      "        [-6.8887e-01],\n",
      "        [ 4.5677e-01],\n",
      "        [ 7.6222e-01],\n",
      "        [-6.2436e-01],\n",
      "        [-5.7546e-02],\n",
      "        [-9.3449e-01],\n",
      "        [-1.4552e+00],\n",
      "        [ 2.1475e+00],\n",
      "        [-2.4729e+00],\n",
      "        [-4.2737e-01],\n",
      "        [ 6.9552e-01],\n",
      "        [-8.4436e-01],\n",
      "        [ 9.9349e-01],\n",
      "        [ 1.8760e+00],\n",
      "        [ 9.4795e-02],\n",
      "        [ 2.1728e-01],\n",
      "        [ 8.1268e-01],\n",
      "        [ 1.7657e+00],\n",
      "        [-2.1114e+00],\n",
      "        [-6.0630e-01],\n",
      "        [ 1.4174e+00],\n",
      "        [-1.6222e+00],\n",
      "        [-7.6931e-01],\n",
      "        [-6.0485e-01],\n",
      "        [-3.0606e-02],\n",
      "        [-2.5105e+00],\n",
      "        [-1.4967e-01],\n",
      "        [-1.1725e+00],\n",
      "        [ 2.0973e+00],\n",
      "        [-8.7585e-01],\n",
      "        [-9.8381e-04],\n",
      "        [ 6.6742e-01],\n",
      "        [-2.4499e-01],\n",
      "        [ 2.0750e+00],\n",
      "        [ 1.7532e+00],\n",
      "        [-6.2346e-01],\n",
      "        [ 1.8531e+00],\n",
      "        [-4.5100e-02],\n",
      "        [ 1.4286e+00],\n",
      "        [-1.0367e+00],\n",
      "        [-9.9222e-01],\n",
      "        [ 7.6589e-01],\n",
      "        [ 5.7311e-01],\n",
      "        [ 5.0022e-01],\n",
      "        [-1.3684e+00],\n",
      "        [ 5.8591e-01],\n",
      "        [ 1.9603e+00],\n",
      "        [ 2.4532e-01],\n",
      "        [-1.4287e+00],\n",
      "        [ 6.0551e-02],\n",
      "        [ 1.3848e+00],\n",
      "        [ 4.9475e-01],\n",
      "        [-2.3969e-01],\n",
      "        [ 6.2297e-01],\n",
      "        [ 5.8004e-01],\n",
      "        [ 1.0625e+00],\n",
      "        [-8.8254e-01],\n",
      "        [-2.8502e-01],\n",
      "        [ 3.3170e-01],\n",
      "        [ 6.2178e-02],\n",
      "        [ 4.6893e-01],\n",
      "        [ 7.1640e-01],\n",
      "        [-1.1036e+00],\n",
      "        [ 3.5125e-01],\n",
      "        [-3.2775e-01],\n",
      "        [ 5.7014e-01],\n",
      "        [-5.4308e-01],\n",
      "        [ 5.8688e-01],\n",
      "        [-4.0135e-01],\n",
      "        [ 4.9933e-02],\n",
      "        [-1.0924e+00],\n",
      "        [ 1.2592e-01]]), tensor([[0.8897],\n",
      "        [0.5303],\n",
      "        [0.1432],\n",
      "        [0.5045],\n",
      "        [0.7305],\n",
      "        [0.2770],\n",
      "        [0.6869],\n",
      "        [0.3541],\n",
      "        [0.1653],\n",
      "        [0.8701],\n",
      "        [0.9753],\n",
      "        [0.6287],\n",
      "        [0.5099],\n",
      "        [0.7125],\n",
      "        [0.6537],\n",
      "        [0.0079],\n",
      "        [0.3878],\n",
      "        [0.2009],\n",
      "        [0.2442],\n",
      "        [0.8373],\n",
      "        [0.1757],\n",
      "        [0.2119],\n",
      "        [0.5837],\n",
      "        [0.5316],\n",
      "        [0.9793],\n",
      "        [0.7473],\n",
      "        [0.8397],\n",
      "        [0.2212],\n",
      "        [0.3984],\n",
      "        [0.0384],\n",
      "        [0.3549],\n",
      "        [0.2669],\n",
      "        [0.5734],\n",
      "        [0.0975],\n",
      "        [0.0160],\n",
      "        [0.9559],\n",
      "        [0.5943],\n",
      "        [0.5473],\n",
      "        [0.2941],\n",
      "        [0.4540],\n",
      "        [0.8153],\n",
      "        [0.8699],\n",
      "        [0.6546],\n",
      "        [0.5362],\n",
      "        [0.0267],\n",
      "        [0.4461],\n",
      "        [0.5054],\n",
      "        [0.3077],\n",
      "        [0.0747],\n",
      "        [0.9647],\n",
      "        [0.8002],\n",
      "        [0.5458],\n",
      "        [0.4113],\n",
      "        [0.5987],\n",
      "        [0.1582],\n",
      "        [0.1407],\n",
      "        [0.6129],\n",
      "        [0.3618],\n",
      "        [0.9243],\n",
      "        [0.8674],\n",
      "        [0.1266],\n",
      "        [0.9109],\n",
      "        [0.1888],\n",
      "        [0.2705],\n",
      "        [0.1276],\n",
      "        [0.4981],\n",
      "        [0.7431],\n",
      "        [0.4146],\n",
      "        [0.4353],\n",
      "        [0.9966],\n",
      "        [0.4299],\n",
      "        [0.1166],\n",
      "        [0.7008],\n",
      "        [0.7532],\n",
      "        [0.6652],\n",
      "        [0.6266],\n",
      "        [0.3461],\n",
      "        [0.8288],\n",
      "        [0.2919],\n",
      "        [0.6059],\n",
      "        [0.4991],\n",
      "        [0.7615],\n",
      "        [0.2324],\n",
      "        [0.4660],\n",
      "        [0.1346],\n",
      "        [0.9217],\n",
      "        [0.8073],\n",
      "        [0.3967],\n",
      "        [0.9500],\n",
      "        [0.5982],\n",
      "        [0.9056],\n",
      "        [0.8746],\n",
      "        [0.7777],\n",
      "        [0.0334],\n",
      "        [0.0820],\n",
      "        [0.5280],\n",
      "        [0.2081],\n",
      "        [0.6784],\n",
      "        [0.1183],\n",
      "        [0.9990],\n",
      "        [0.5168],\n",
      "        [0.3130],\n",
      "        [0.1163],\n",
      "        [0.4808],\n",
      "        [0.3006],\n",
      "        [0.3400],\n",
      "        [0.8193],\n",
      "        [0.2601],\n",
      "        [0.4001],\n",
      "        [0.0510],\n",
      "        [0.0815],\n",
      "        [0.8130],\n",
      "        [0.2302],\n",
      "        [0.7900],\n",
      "        [0.9313],\n",
      "        [0.3343],\n",
      "        [0.6435],\n",
      "        [0.7472],\n",
      "        [0.6635],\n",
      "        [0.6364],\n",
      "        [0.1678],\n",
      "        [0.7642],\n",
      "        [0.7511],\n",
      "        [0.5798],\n",
      "        [0.8746],\n",
      "        [0.9233],\n",
      "        [0.4515],\n",
      "        [0.5929],\n",
      "        [0.2947],\n",
      "        [0.3316],\n",
      "        [0.3401],\n",
      "        [0.7469],\n",
      "        [0.1256],\n",
      "        [0.1737],\n",
      "        [0.1731],\n",
      "        [0.6327],\n",
      "        [0.4346],\n",
      "        [0.2077],\n",
      "        [0.1689],\n",
      "        [0.0654],\n",
      "        [0.0124],\n",
      "        [0.2467],\n",
      "        [0.9218],\n",
      "        [0.7081],\n",
      "        [0.3424],\n",
      "        [0.8037],\n",
      "        [0.5370],\n",
      "        [0.4272],\n",
      "        [0.1812],\n",
      "        [0.4695],\n",
      "        [0.5542],\n",
      "        [0.8773],\n",
      "        [0.7182],\n",
      "        [0.5867],\n",
      "        [0.3230],\n",
      "        [0.3128],\n",
      "        [0.8758],\n",
      "        [0.2718],\n",
      "        [0.4040],\n",
      "        [0.7448],\n",
      "        [0.3122],\n",
      "        [0.8772],\n",
      "        [0.2463],\n",
      "        [0.9568],\n",
      "        [0.2017],\n",
      "        [0.2312],\n",
      "        [0.1510],\n",
      "        [0.9851],\n",
      "        [0.9935],\n",
      "        [0.2189],\n",
      "        [0.6849],\n",
      "        [0.4014],\n",
      "        [0.0376],\n",
      "        [0.2694],\n",
      "        [0.6902],\n",
      "        [0.2236],\n",
      "        [0.8965],\n",
      "        [0.6308],\n",
      "        [0.3036],\n",
      "        [0.8425],\n",
      "        [0.4562],\n",
      "        [0.7025],\n",
      "        [0.4125],\n",
      "        [0.5557],\n",
      "        [0.3888],\n",
      "        [0.2357],\n",
      "        [0.8357],\n",
      "        [0.2350],\n",
      "        [0.2130],\n",
      "        [0.9543],\n",
      "        [0.6571],\n",
      "        [0.6143],\n",
      "        [0.7570],\n",
      "        [0.4505],\n",
      "        [0.3720],\n",
      "        [0.2267],\n",
      "        [0.7964],\n",
      "        [0.5901],\n",
      "        [0.1247],\n",
      "        [0.9456]])]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#Define um conjunto de dados a partir da classe Dados para treinamento e teste\n",
    "training_dataset = Dados(0, 1, 1000)\n",
    "test_dataset = Dados(0,1,100)\n",
    "\n",
    "#Define os dataloaders para treinamento e teste\n",
    "train_dataloader = DataLoader(training_dataset, batch_size = 200, shuffle = True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = 20, shuffle = True)\n",
    "\n",
    "\n",
    "# Cria um iterador para o DataLoader\n",
    "data_iter = iter(train_dataloader)\n",
    "\n",
    "# Obtém o primeiro lote de dados\n",
    "first_batch_samples, first_batch_targets = next(data_iter)\n",
    "\n",
    "# Imprime o formato do primeiro lote de dados\n",
    "print(first_batch_samples.shape)\n",
    "print(first_batch_targets.shape)\n",
    "\n",
    "for i, data in enumerate(train_dataloader):\n",
    "    print(f\"Batch: {i}, Data: {data}\")\n",
    "    if i > 10:  # Limitar a quantidade de saída\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f82c56",
   "metadata": {},
   "source": [
    "<a id=\"heading--3\"></a>\n",
    "## Construindo uma Rede Neural\n",
    "\n",
    "As redes neurais são modelos computacionais, formadas por camadas de neurônios artificiais, estruturas que realizam operações paramétricas com os dados de entrada. O namespace __torch.nn__ fornece todos os blocos necessários para a construção de uma rede neural.\n",
    "\n",
    "Todo módulo no PyTorch é uma subclasse do __nn.Module__, e a rede é basicamente um módulo composto por outros módulos. Aqui, será desenvolvido um modelo de regressão para aproximação de valores aleatórios, com distribuição de probabilidade Gaussiana.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1ce3d8",
   "metadata": {},
   "source": [
    "<a id=\"heading--3-1\"></a>\n",
    "### Definindo a Classe\n",
    "\n",
    "Nós definimos as camadas da nossa rede neural na função **_ _ init _ _**, e toda subclasse do módulo implementa as operações de entrada de dados no método forward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd957bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "        nn.Linear(1, 10),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(10,5),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(5,1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.linear_relu_stack(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e9f719",
   "metadata": {},
   "source": [
    "<a id=\"heading--3-2\"></a>\n",
    "### Camadas e Parâmetros\n",
    "\n",
    "As camadas da rede neural são aplicadas no método init, geralmente como parâmetros para a função Sequential, que executa as camadas na ordem em que são dadas.\n",
    "O módulo fornece, além de funções lineares, e diversas funções de ativação, como a **ReLU**, que representão não linearidades na nossa rede neural. Essas funções de ativação são necessárias para a aproximação de sistemas não lineares.\n",
    "\n",
    "Essas camadas possuem parâmetros que são utilizados para a aproximação de funções a partir dos dados de entrada, pesos são multiplicados e biases são somados a eles. Esses parâmetros podem ser acessados usando os métodos parameters() ou named_parameters() do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffff3768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estrutura do modelo: tensor([[0.2358],\n",
      "        [0.2169],\n",
      "        [0.2054],\n",
      "        [0.2115],\n",
      "        [0.2117],\n",
      "        [0.2115],\n",
      "        [0.2085],\n",
      "        [0.2185],\n",
      "        [0.2058],\n",
      "        [0.2286]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([10, 1]) | Values : tensor([[-0.5531],\n",
      "        [-0.6762]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([10]) | Values : tensor([-0.3908, -0.7253], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([5, 10]) | Values : tensor([[ 0.2604, -0.2064, -0.2074,  0.2656,  0.2973,  0.2744,  0.0609,  0.0482,\n",
      "          0.1465,  0.0844],\n",
      "        [-0.0296,  0.1473,  0.0841, -0.0370, -0.1730, -0.0518,  0.0763, -0.0516,\n",
      "          0.1126, -0.2061]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([5]) | Values : tensor([-0.2728, -0.1454], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([1, 5]) | Values : tensor([[-0.2313,  0.3913,  0.0936, -0.2834,  0.3282]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([1]) | Values : tensor([0.1429], grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(10, 1)\n",
    "model = NN()\n",
    "Y = model(X)\n",
    "print(f\"Estrutura do modelo: {Y}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61eec13c",
   "metadata": {},
   "source": [
    "<a id=\"heading--4\"></a>\n",
    "## Diferenciação Automática com __torch.autograd__\n",
    "\n",
    "O algoritmo mais utlizado no treinamento de redes neurais é a retro-propagação, na qual os parâmetros associados à entrada são  ajustados a partir do gradiente da função de perda associada a cada parâmetro. \n",
    "Para computar os gradientes de cada variável nos ativamos sua propriedade *requires_grad para True*\n",
    "\n",
    "O código abaixo aplica uma camada linear à entrada, sendo w o peso associado aos dados e b o bias somado ao resultado. A função de perda é dada por loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76ac8ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(5)  # tensor de entrada\n",
    "y = torch.zeros(3)  # saída desejada\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7b1d12",
   "metadata": {},
   "source": [
    "<a id=\"heading--4-1\"></a>\n",
    "### Computando Gradientes\n",
    "\n",
    "O método loss.backwards() calcula as derivadas parcias da perda com relação a **w** e a **b**, os nossos parâmetros, aplicados para um **x** e **y** constantes. Nós então podemos obter esses gradietes a partir da função *tensor.grad*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5aa5a790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0055, 0.0161, 0.1021],\n",
      "        [0.0055, 0.0161, 0.1021],\n",
      "        [0.0055, 0.0161, 0.1021],\n",
      "        [0.0055, 0.0161, 0.1021],\n",
      "        [0.0055, 0.0161, 0.1021]])\n",
      "tensor([0.0055, 0.0161, 0.1021])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd625bc",
   "metadata": {},
   "source": [
    "<a id=\"heading--4-2\"></a>\n",
    "### Desabilitando o Rastreamento dos Gradientes\n",
    "\n",
    "Por padrão, todos os tensores já vem com o seu requires_grad = True, e rastreiam o seu histórico computacional e suporte a computação dos gradientes. Em certas situações, não é desejável que os gradientes sejam associados aos parâmetros, há somente a necessidade de testar o método forward da rede. \n",
    "\n",
    "É possível desativar o requires_grad de um tensor de agumas formas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a75da5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "\n",
      "Antes do detach:  True\n",
      "\n",
      "Depois do detach:  False\n"
     ]
    }
   ],
   "source": [
    "#Utilizando o bloco torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "\n",
    "print(z.requires_grad)\n",
    "\n",
    "#Utilizando o método detach():\n",
    "z = torch.matmul(x, w)+b\n",
    "print(\"\\nAntes do detach: \", z.requires_grad)\n",
    "z_det = z.detach()\n",
    "print(\"\\nDepois do detach: \", z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cb5068",
   "metadata": {},
   "source": [
    "<a id=\"heading--5\"></a>\n",
    "## Treinando e Testando o Modelo\n",
    "\n",
    "Agora que já temos um modelo de rede neural, o próximo passo é treinar, avaliar e testar o nosso modelo otimizando os parâmetros dos nossos dados. No processo iterativo de treinamento, cada iteração atribui parâmetros iniciais aos dados, calucla a função de erro, e ajusta os parâmetros iniciais a partir da derivada da função de perda em relação aos parâmetros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce22d92",
   "metadata": {},
   "source": [
    "<a id=\"heading--5-1\"></a>\n",
    "### HiperParâmetros\n",
    "\n",
    "O processo de otimização pode ser controlado utilizando o que chamamos de hiperparâmetros, que podem impacctar no treinamento do modelo e na taxa de convergência. Os hiperparâmetros que são utilizáveis são:\n",
    "\n",
    "__Número de épocas__:  A quantidade de vezes para iterar sobre os dados;\n",
    "\n",
    "__Tamanho do lote__: O número de amostras que serão passadas ao modelo antes da atualização dos parâmetros;\n",
    "\n",
    "**Taxa de aprendizado**: O quanto atualizar os parâmetros a cada lote ou época. Valores pequenos podem gerar um processo muito lento, mas valores altos podem  causar comportamentos imprevisíveis.\n",
    "\n",
    "Cada época consiste de duas principais partes:\n",
    "\n",
    "*Loop de treino*: itera sobre o dataset de treinamento e tenta alcançar os parâmetros ótimos;\n",
    "\n",
    "*Loop de teste*: Itera sobre o dataset de teste para avaliar o aprendizado do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1f9d902",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_rate = 1e-2\n",
    "batch = len(test_dataloader)\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfacc07c",
   "metadata": {},
   "source": [
    "<a id=\"heading--5-2\"></a>\n",
    "### Funções de Perda e Otimizador\n",
    "\n",
    "As funções de perda são responsáveis por medir  o grau de erro entre os resultados obtidos pelo modelo e o conjunto de dados alvo. É justamente essa função que desejamos minimizar durante o treinamento.\n",
    "\n",
    "O torch disponibiliza diferentes tipos de funções de perda, para diferenes problemas: *nn.MSELoss*, para problemas de regressão, *nn.NLLLoss*, para problemas de classificação, *nn.CrossEntropyLoss*, que vai combinar a função de classificação com o método de probabilidade *nn.LogSoftMax*\n",
    "\n",
    "Para nosso modelo de regressão, utilizaremos a MSELoss.\n",
    "\n",
    "Os algoritmos de otimização definem como o processo de ajuste dos parâmetros será feito. existem diversos orimizadores disponíveis na biblioteca do Pytorch. No nosso exemplo, utilizaremos o otimizador Adam, que combina os processos SGD(*Stochastic Gradient Descent*) e RMSProp().\n",
    "\n",
    "Nós inicializamos o otimizador ao registrar os parâmetros  que precisam ser treinados, e passando a taxa de aprendizado como hiperparâmetro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4081e478",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr =  learn_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1ea886",
   "metadata": {},
   "source": [
    "Dentro do loop de treino, a otimização ocorre em três etapas:\n",
    "\n",
    "* Chamada do método optimizer.zero_grad() para resetar os gradientes. Por padrão, esses gradientes são somados a cada iteração como isso não é desejável em nosso caso, nós os setamos para 0 a cada iteração;\n",
    " \n",
    " \n",
    "* Retropropagamos a função de perda com loss.backward(), que deposita os gradientes nos respectivos parâmetros;\n",
    "\n",
    "\n",
    "* Chamada do optimizer.step(), para ajustar os parâmetros com os gradientes obtidos no passo anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80148063",
   "metadata": {},
   "source": [
    "<a id=\"heading--5-3\"></a>\n",
    "### Implementação\n",
    "\n",
    "Aqui, definimos *train_loop*, que reitera sobre nosso código de otimização, e *test_loop*, que avalia o nosso modelo a partir dos nossos dados de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a5f3198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    \n",
    "    model.train()\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "        predict = model(x)\n",
    "        loss_value = loss(predict, y)\n",
    "        \n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if batch % 1 == 0:\n",
    "            loss_value, current = loss_value.item(), (batch+1)*len(x)\n",
    "            print(f\"loss: {loss_value:>7f}[{current:>5d}/{size:>5d}]\" )\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss):\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss =0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            predict = model(x)\n",
    "            test_loss += loss(predict, y).item()\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Perda média: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "baa25573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.167467[  200/ 1000]\n",
      "loss: 0.177077[  400/ 1000]\n",
      "loss: 0.125214[  600/ 1000]\n",
      "loss: 0.126209[  800/ 1000]\n",
      "loss: 0.120535[ 1000/ 1000]\n",
      "Perda média: 0.112350 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.097789[  200/ 1000]\n",
      "loss: 0.105444[  400/ 1000]\n",
      "loss: 0.084260[  600/ 1000]\n",
      "loss: 0.096649[  800/ 1000]\n",
      "loss: 0.101720[ 1000/ 1000]\n",
      "Perda média: 0.104148 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.085985[  200/ 1000]\n",
      "loss: 0.090928[  400/ 1000]\n",
      "loss: 0.087269[  600/ 1000]\n",
      "loss: 0.086458[  800/ 1000]\n",
      "loss: 0.092727[ 1000/ 1000]\n",
      "Perda média: 0.100709 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.089251[  200/ 1000]\n",
      "loss: 0.086766[  400/ 1000]\n",
      "loss: 0.078078[  600/ 1000]\n",
      "loss: 0.084422[  800/ 1000]\n",
      "loss: 0.083376[ 1000/ 1000]\n",
      "Perda média: 0.095413 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.088626[  200/ 1000]\n",
      "loss: 0.084103[  400/ 1000]\n",
      "loss: 0.082385[  600/ 1000]\n",
      "loss: 0.078797[  800/ 1000]\n",
      "loss: 0.075734[ 1000/ 1000]\n",
      "Perda média: 0.093701 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.082533[  200/ 1000]\n",
      "loss: 0.098299[  400/ 1000]\n",
      "loss: 0.074820[  600/ 1000]\n",
      "loss: 0.079912[  800/ 1000]\n",
      "loss: 0.074217[ 1000/ 1000]\n",
      "Perda média: 0.093237 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.076698[  200/ 1000]\n",
      "loss: 0.091051[  400/ 1000]\n",
      "loss: 0.083315[  600/ 1000]\n",
      "loss: 0.067479[  800/ 1000]\n",
      "loss: 0.087563[ 1000/ 1000]\n",
      "Perda média: 0.093655 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.078549[  200/ 1000]\n",
      "loss: 0.079377[  400/ 1000]\n",
      "loss: 0.082525[  600/ 1000]\n",
      "loss: 0.083278[  800/ 1000]\n",
      "loss: 0.080754[ 1000/ 1000]\n",
      "Perda média: 0.094801 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.081513[  200/ 1000]\n",
      "loss: 0.075849[  400/ 1000]\n",
      "loss: 0.089998[  600/ 1000]\n",
      "loss: 0.080076[  800/ 1000]\n",
      "loss: 0.077574[ 1000/ 1000]\n",
      "Perda média: 0.094772 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.075860[  200/ 1000]\n",
      "loss: 0.082231[  400/ 1000]\n",
      "loss: 0.078242[  600/ 1000]\n",
      "loss: 0.079968[  800/ 1000]\n",
      "loss: 0.088008[ 1000/ 1000]\n",
      "Perda média: 0.093780 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss, optimizer)\n",
    "    test_loop(test_dataloader, model, loss)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cd1941",
   "metadata": {},
   "source": [
    "Aqui nós podemos observar dois comportamentos interessantes na perda:\n",
    "\n",
    "* Ela aparenta decrescer em média de forma lenta. Isso se deve muito provavelmente de que estamos tratando de um problema de regressão, com valores aleatórios, e é extremamente complicado que a rede neural consiga aprender a resolver esse problema.\n",
    "\n",
    "* O outro comportamento, se refere às oscilações da perda média, observe que ela decresce e cresce em alguns momentos. Isso provavelmente se deve ao nosso learning_rate, que deve estar alto demais, e causando essass oscilações."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2758c210",
   "metadata": {},
   "source": [
    "<a id=\"heading--5-4\"></a>\n",
    "### Salvando e Carregando Modelos\n",
    "\n",
    "É possível salvar os parâmetros aprendidos pelo modelo em um dicionário de estado state_dictonary, extermnamente, e podem ser carregados para um anova instância do mesmo modelo:\n",
    "\n",
    "Além disso, pode-se salvar o próprio modelo, considerando sua estrutura, alés dos pesos que foram calculados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c3560ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Salva os parâmetros do modelo\n",
    "torch.save(model.state_dict(), 'model_weights.pth')\n",
    "\n",
    "#Salva a aestrutura do modelo\n",
    "torch.save(model, 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82549215",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carregando os parâmetros do modelo\n",
    "model2 = NN()\n",
    "model.load_state_dict(torch.load('model_weights.pth'))\n",
    "model.eval()\n",
    "\n",
    "#Carregando o modelo em si\n",
    "model3 = torch.load('model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
